{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "revised-building",
   "metadata": {},
   "source": [
    "This Jupyter notebook contains a set of sample codes that are necessary for obtaining the calculation results in the paper titled \"Functional Output Regression for Machine Learning in Materials Science\". For readers who want to test the model without training the model from scratch, we provide three pre-trained models that are available for download: https://github.com/yoshida-lab/XenonPy/releases/download/v0.6.3/pretrained_models.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "innovative-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python packages used in the sample codes\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from rdkit.Avalon import pyAvalonTools\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit import rdBase, Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.AtomPairs import Pairs, Torsions\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# user-friendly print out\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-james",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-impossible",
   "metadata": {},
   "source": [
    "#### Utility function: define RBFKernel class to be the radial basis function kernel that will be used in our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "encouraging-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import Union, Sequence, Callable, Tuple\n",
    "\n",
    "\n",
    "class RBFKernel():\n",
    "    def __init__(self, sigmas_squared: Union[float, int, np.ndarray, Sequence], hight=10, *, dtype='float32'):\n",
    "        \"\"\"\n",
    "        Radial Basis Function (RBF) kernel function.\n",
    "        Ref: https://en.wikipedia.org/wiki/Radial_basis_function_kernel\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sigmas:\n",
    "            The squared standard deviations (SD).\n",
    "            Can be a single number or a 1d array-like object.\n",
    "        x_i: np.ndarray\n",
    "            Should be a 1d array.\n",
    "        x_j: np.ndarray\n",
    "            Should be a 1d array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Distribution under RBF kernel.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            Raise error if sigmas has wrong dimension.\n",
    "        \"\"\"\n",
    "        sigmas_squared = np.asarray(sigmas_squared)\n",
    "        if sigmas_squared.ndim == 0:\n",
    "            sigmas_squared = sigmas_squared[np.newaxis]\n",
    "        if sigmas_squared.ndim != 1:\n",
    "            raise ValueError('parameter `sigmas_squared` must be a array-like object which has dimension 1')\n",
    "        self._sigmas_squared = sigmas_squared\n",
    "        self.hight = hight\n",
    "        self.dtype = dtype\n",
    "        \n",
    "    @property\n",
    "    def sigmas_squared(self):\n",
    "        return np.copy(self._sigmas_squared)\n",
    "    \n",
    "    def __call__(self, x_i: np.ndarray, x_j: Union[np.ndarray, int, float]):\n",
    "        # K(x_i, x_j) = exp(-||x_i - x_j||^2 / (2 * sigma^2))\n",
    "        p1 = np.power(np.expand_dims(x_i, axis=x_i.ndim) - x_j, 2)\n",
    "        p2 = self._sigmas_squared * 2\n",
    "        dists = self.hight * np.exp(-np.expand_dims(p1, axis=p1.ndim) / p2)\n",
    "\n",
    "        if dists.shape[2] == 1:\n",
    "            return dists[:, :, 0].astype(self.dtype)\n",
    "        return dists.astype(self.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-department",
   "metadata": {},
   "source": [
    "#### Model definition: define KernelNet class to be the neural network based kernel function model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "religious-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xenonpy.model import SequentialLinear\n",
    "\n",
    "# model parameter initialization\n",
    "@torch.no_grad()\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        \n",
    "class KernelNet(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                 n_neurons=(1024, 896, 512, 256, 128),\n",
    "                 activation_func=nn.LeakyReLU(0.2, inplace=True),\n",
    "                 kernel_grids: Union[int, np.ndarray] = 128,\n",
    "                 wavelength_points: Union[int, np.ndarray] =181,\n",
    "                 kernel_func=RBFKernel(sigmas_squared=0.05, hight=1),\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        # initializing fixed params (not updated through training)\n",
    "        if isinstance(kernel_grids, int):\n",
    "            kernel_grids = np.linspace(0, 1, kernel_grids)\n",
    "        if isinstance(kernel_grids, np.ndarray):\n",
    "            self.kernel_grids = kernel_grids\n",
    "        else:\n",
    "            raise ValueError('kernel_grids error!')\n",
    "        \n",
    "        if isinstance(wavelength_points, int):\n",
    "            wavelength_points = np.linspace(0, 1, wavelength_points)\n",
    "        if isinstance(wavelength_points, np.ndarray):\n",
    "            self.wavelength_points = wavelength_points\n",
    "        else:\n",
    "            raise ValueError('wavelength_points error!')\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc_layers = SequentialLinear(\n",
    "            in_features=n_neurons[0], out_features=n_neurons[-1], h_neurons=n_neurons[1:-1], h_activation_funcs=activation_func\n",
    "        )\n",
    "        \n",
    "        # baseline parameters\n",
    "        self.mu = torch.nn.Parameter(torch.zeros(wavelength_points.size))\n",
    "        \n",
    "        # kernel\n",
    "        self.kernel = torch.from_numpy(\n",
    "            kernel_func(self.kernel_grids, self.wavelength_points)\n",
    "        )\n",
    "        \n",
    "    def to(self, *arg, **kwarg):\n",
    "        self.kernel = self.kernel.to(*arg, **kwarg)\n",
    "        return super().to(*arg, **kwarg)\n",
    "\n",
    "    def forward(self, fingerprint_input):\n",
    "        x = self.fc_layers(fingerprint_input)\n",
    "        x = x.view(x.size(0), self.kernel_grids.size, -1) \n",
    "        x = x * self.kernel\n",
    "        x = torch.sum(x, dim=1)\n",
    "        x = self.mu + x\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-appraisal",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "vocal-hamburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data: 949\n"
     ]
    }
   ],
   "source": [
    "# load csv file\n",
    "data_file = \"data/Dataset_I.csv\"\n",
    "input_file = pd.read_csv(data_file,sep=\",\")\n",
    "\n",
    "# Figure\n",
    "out_result = Path(\"spectrum_results\")\n",
    "out_result.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# drop \"OC(=O)C(=C\\C1=CC=[Cl]C=[Cl]1)\\C1=CN=CC=C1\" because of Explicit valence error\n",
    "input_file = input_file[input_file.SMILES != \"OC(=O)C(=C\\C1=CC=[Cl]C=[Cl]1)\\C1=CN=CC=C1\"]\n",
    "\n",
    "# load smiles and spectrum data\n",
    "file_smiles_spectrum = input_file.values\n",
    "smiles_list = (file_smiles_spectrum[:,1])\n",
    "spectrum_list = file_smiles_spectrum[:,2:]\n",
    "spectrum_list = np.array(spectrum_list, dtype='float32')\n",
    "\n",
    "print(\"number of data:\", len(smiles_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-image",
   "metadata": {},
   "source": [
    "#### convert smiles to fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "extended-champion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([949, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([949, 182])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert smiles to mol file\n",
    "mol_list = [Chem.MolFromSmiles(s) for s in smiles_list if s is not None]\n",
    "    \n",
    "## Morgan fingerprint (ECFP) with radius=3 and 1024 bits\n",
    "fps_bit = [AllChem.GetMorganFingerprintAsBitVect(m, 3, 1024) for m in mol_list]\n",
    "fps_list = []\n",
    "for fps in fps_bit:\n",
    "    fps_arr = np.zeros((1,))\n",
    "    DataStructs.ConvertToNumpyArray(fps, fps_arr)\n",
    "    fps_list.append(fps_arr)\n",
    "fps_list = np.array(fps_list, dtype='float32')\n",
    "\n",
    "fps_list_max = np.amax(fps_list)\n",
    "fps_list_min = np.amin(fps_list)\n",
    "fps_norm = ((fps_list - fps_list_min) / (fps_list_max - fps_list_min))\n",
    "X_data = torch.from_numpy(fps_norm)\n",
    "\n",
    "\n",
    "spectrum_list_max = np.amax(spectrum_list)\n",
    "spectrum_list_min = np.amin(spectrum_list)\n",
    "spectrum_norm = ((spectrum_list - spectrum_list_min) / (spectrum_list_max - spectrum_list_min))\n",
    "Y_data = torch.from_numpy(spectrum_norm)\n",
    "data_number = torch.zeros((X_data.shape[0],1))\n",
    "Y_data = torch.cat((data_number, Y_data), dim=1)\n",
    "for i in range(Y_data.shape[0]):\n",
    "    number = torch.tensor([i]) \n",
    "    Y_data[i][0] = number\n",
    "    \n",
    "X_data.shape\n",
    "Y_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-cocktail",
   "metadata": {},
   "source": [
    "### Set up hyper-params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-punishment",
   "metadata": {},
   "source": [
    "#### 1. model training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "consecutive-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 3000 # epoch\n",
    "lr = 0.0002 # learning rate\n",
    "ngpu = 2 # number of GPU\n",
    "display_interval = 100\n",
    "\n",
    "model_path = Path('model/dataset1')\n",
    "model_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "fatty-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "best_layer_number = 4 # number of layers\n",
    "best_variance = 0.0005 # the variange of RBF kernel\n",
    "best_length = 0.5 # the length of RBF kernel \n",
    "\n",
    "# split train:val:test\n",
    "split_size = 0.316\n",
    "split_val_size = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "unusual-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_layer = best_layer_number\n",
    "end_layer = start_layer + 1\n",
    "\n",
    "hyper_id = [5, 6, 7, 8]\n",
    "hyper_order = 3\n",
    "start_hyper = hyper_id[hyper_order]\n",
    "end_hyper = start_hyper + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-defeat",
   "metadata": {},
   "source": [
    "#### 2. model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "unnecessary-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength_points = 181\n",
    "kernel_grids = 128\n",
    "n_neurons = [\n",
    "    (1024, 512, 128),                 # 1 hidden layer\n",
    "    (1024, 512, 256, 128),            # 2 hidden layer\n",
    "    (1024, 896, 512, 256, 128),       # 3 hidden layer\n",
    "    (1024, 896, 640, 512, 256, 128),  # 4 hidden layer\n",
    "]\n",
    "kernel_hypers = (\n",
    "    [10, 0.00005], [5, 0.00005], [1, 0.00005], [0.5, 0.00005],\n",
    "    [10, 0.0005], [5, 0.0005], [1, 0.0005], [0.5, 0.0005],\n",
    "    [10, 0.00025], [5, 0.00025], [1, 0.00025], [0.5, 0.00025],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-swaziland",
   "metadata": {},
   "source": [
    "### Begin model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "executive-buffer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x145f7344f1f0>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fixing random seeds for reproducing the results\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-style",
   "metadata": {},
   "source": [
    "#### train with different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "passive-links",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNet(\n",
       "  (fc_layers): SequentialLinear(\n",
       "    (layer_0): LinearLayer(\n",
       "      (linear): Linear(in_features=1024, out_features=896, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_1): LinearLayer(\n",
       "      (linear): Linear(in_features=896, out_features=640, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_2): LinearLayer(\n",
       "      (linear): Linear(in_features=640, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_3): LinearLayer(\n",
       "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (output): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 4 hidden layer model with hyper_number 8 and random_number 0...\n",
      "[1/3000][1/13] Loss: 0.9513877 \n",
      "[101/3000][1/13] Loss: 0.0049158 \n",
      "[201/3000][1/13] Loss: 0.0015354 \n",
      "[301/3000][1/13] Loss: 0.0025721 \n",
      "[401/3000][1/13] Loss: 0.0016615 \n",
      "[501/3000][1/13] Loss: 0.0015076 \n",
      "[601/3000][1/13] Loss: 0.0017643 \n",
      "[701/3000][1/13] Loss: 0.0011007 \n",
      "[801/3000][1/13] Loss: 0.0012107 \n",
      "[901/3000][1/13] Loss: 0.0005568 \n",
      "[1001/3000][1/13] Loss: 0.0009820 \n",
      "[1101/3000][1/13] Loss: 0.0014051 \n",
      "[1201/3000][1/13] Loss: 0.0009783 \n",
      "[1301/3000][1/13] Loss: 0.0006637 \n",
      "[1401/3000][1/13] Loss: 0.0008219 \n",
      "[1501/3000][1/13] Loss: 0.0006080 \n",
      "[1601/3000][1/13] Loss: 0.0007286 \n",
      "[1701/3000][1/13] Loss: 0.0009694 \n",
      "[1801/3000][1/13] Loss: 0.0008945 \n",
      "[1901/3000][1/13] Loss: 0.0005525 \n",
      "[2001/3000][1/13] Loss: 0.0005456 \n",
      "[2101/3000][1/13] Loss: 0.0008501 \n",
      "[2201/3000][1/13] Loss: 0.0004946 \n",
      "[2301/3000][1/13] Loss: 0.0003780 \n",
      "[2401/3000][1/13] Loss: 0.0005973 \n",
      "[2501/3000][1/13] Loss: 0.0005007 \n",
      "[2601/3000][1/13] Loss: 0.0006730 \n",
      "[2701/3000][1/13] Loss: 0.0005046 \n",
      "[2801/3000][1/13] Loss: 0.0003679 \n",
      "[2901/3000][1/13] Loss: 0.0004459 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GNet(\n",
       "  (fc_layers): SequentialLinear(\n",
       "    (layer_0): LinearLayer(\n",
       "      (linear): Linear(in_features=1024, out_features=896, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_1): LinearLayer(\n",
       "      (linear): Linear(in_features=896, out_features=640, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_2): LinearLayer(\n",
       "      (linear): Linear(in_features=640, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_3): LinearLayer(\n",
       "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (output): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 4 hidden layer model with hyper_number 8 and random_number 1...\n",
      "[1/3000][1/13] Loss: 0.8627182 \n",
      "[101/3000][1/13] Loss: 0.0093594 \n",
      "[201/3000][1/13] Loss: 0.0030646 \n",
      "[301/3000][1/13] Loss: 0.0041787 \n",
      "[401/3000][1/13] Loss: 0.0021683 \n",
      "[501/3000][1/13] Loss: 0.0025061 \n",
      "[601/3000][1/13] Loss: 0.0014158 \n",
      "[701/3000][1/13] Loss: 0.0016073 \n",
      "[801/3000][1/13] Loss: 0.0020572 \n",
      "[901/3000][1/13] Loss: 0.0013698 \n",
      "[1001/3000][1/13] Loss: 0.0013439 \n",
      "[1101/3000][1/13] Loss: 0.0011626 \n",
      "[1201/3000][1/13] Loss: 0.0013354 \n",
      "[1301/3000][1/13] Loss: 0.0007292 \n",
      "[1401/3000][1/13] Loss: 0.0008215 \n",
      "[1501/3000][1/13] Loss: 0.0005767 \n",
      "[1601/3000][1/13] Loss: 0.0007712 \n",
      "[1701/3000][1/13] Loss: 0.0009511 \n",
      "[1801/3000][1/13] Loss: 0.0006395 \n",
      "[1901/3000][1/13] Loss: 0.0005922 \n",
      "[2001/3000][1/13] Loss: 0.0004482 \n",
      "[2101/3000][1/13] Loss: 0.0015007 \n",
      "[2201/3000][1/13] Loss: 0.0005241 \n",
      "[2301/3000][1/13] Loss: 0.0005661 \n",
      "[2401/3000][1/13] Loss: 0.0005935 \n",
      "[2501/3000][1/13] Loss: 0.0004629 \n",
      "[2601/3000][1/13] Loss: 0.0005072 \n",
      "[2701/3000][1/13] Loss: 0.0011309 \n",
      "[2801/3000][1/13] Loss: 0.0006281 \n",
      "[2901/3000][1/13] Loss: 0.0004993 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GNet(\n",
       "  (fc_layers): SequentialLinear(\n",
       "    (layer_0): LinearLayer(\n",
       "      (linear): Linear(in_features=1024, out_features=896, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_1): LinearLayer(\n",
       "      (linear): Linear(in_features=896, out_features=640, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_2): LinearLayer(\n",
       "      (linear): Linear(in_features=640, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_3): LinearLayer(\n",
       "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (output): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 4 hidden layer model with hyper_number 8 and random_number 2...\n",
      "[1/3000][1/13] Loss: 1.0644132 \n",
      "[101/3000][1/13] Loss: 0.0099005 \n",
      "[201/3000][1/13] Loss: 0.0022077 \n",
      "[301/3000][1/13] Loss: 0.0020309 \n",
      "[401/3000][1/13] Loss: 0.0017221 \n",
      "[501/3000][1/13] Loss: 0.0014918 \n",
      "[601/3000][1/13] Loss: 0.0014006 \n",
      "[701/3000][1/13] Loss: 0.0013380 \n",
      "[801/3000][1/13] Loss: 0.0011670 \n",
      "[901/3000][1/13] Loss: 0.0013053 \n",
      "[1001/3000][1/13] Loss: 0.0006377 \n",
      "[1101/3000][1/13] Loss: 0.0009901 \n",
      "[1201/3000][1/13] Loss: 0.0008227 \n",
      "[1301/3000][1/13] Loss: 0.0011035 \n",
      "[1401/3000][1/13] Loss: 0.0006691 \n",
      "[1501/3000][1/13] Loss: 0.0009261 \n",
      "[1601/3000][1/13] Loss: 0.0007164 \n",
      "[1701/3000][1/13] Loss: 0.0007345 \n",
      "[1801/3000][1/13] Loss: 0.0006400 \n",
      "[1901/3000][1/13] Loss: 0.0005412 \n",
      "[2001/3000][1/13] Loss: 0.0003890 \n",
      "[2101/3000][1/13] Loss: 0.0004704 \n",
      "[2201/3000][1/13] Loss: 0.0003938 \n",
      "[2301/3000][1/13] Loss: 0.0004376 \n",
      "[2401/3000][1/13] Loss: 0.0005553 \n",
      "[2501/3000][1/13] Loss: 0.0007593 \n",
      "[2601/3000][1/13] Loss: 0.0003920 \n",
      "[2701/3000][1/13] Loss: 0.0003260 \n",
      "[2801/3000][1/13] Loss: 0.0003903 \n",
      "[2901/3000][1/13] Loss: 0.0005001 \n"
     ]
    }
   ],
   "source": [
    "# repeat for different number of layers in the neural network\n",
    "for layer_number in range(start_layer, end_layer):\n",
    "    # repeat for different sigma values for the kernel\n",
    "    for hyper_number in range(start_hyper, end_hyper):\n",
    "        # repeat for three times to check sensitivity to data splitting\n",
    "        for random_number in range(3):\n",
    "            # split data with different random seed\n",
    "            X_train, X_test_val, Y_train, Y_test_val = train_test_split(X_data, Y_data, test_size=split_size, random_state=random_number) \n",
    "            X_test, X_val, Y_test, Y_val = train_test_split(X_test_val, Y_test_val, test_size=split_val_size, random_state=random_number)\n",
    "            Dataset_train = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "            loader_train = torch.utils.data.DataLoader(Dataset_train, batch_size=50, shuffle=True)\n",
    "            Dataset_val = torch.utils.data.TensorDataset(X_val, Y_val)\n",
    "            loader_val = torch.utils.data.DataLoader(Dataset_val, shuffle=False)\n",
    "\n",
    "            # bulid model\n",
    "            netG = KernelNet(\n",
    "                n_neurons=n_neurons[layer_number - 1],\n",
    "                wavelength_points=wavelength_points,\n",
    "                kernel_grids=kernel_grids,\n",
    "                kernel_func=RBFKernel(\n",
    "                    sigmas_squared=kernel_hypers[hyper_number - 1][1],\n",
    "                    hight=kernel_hypers[hyper_number - 1][0]\n",
    "                ),\n",
    "            ).to(device)\n",
    "\n",
    "            # Handle multi-gpu if desired\n",
    "\n",
    "            # if (device.type == 'cuda') and (torch.cuda.device_count() > ngpu):\n",
    "            #     netG = nn.DataParallel(netG, device_ids=range(ngpu))            \n",
    "            netG = netG.apply(weights_init)\n",
    "\n",
    "            # setup optimizer for training model\n",
    "            regu = torch.Tensor([1]).to(device) # parameter regularization\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "            \n",
    "            # train with fixed number of epochs\n",
    "            losses = [] \n",
    "            print(f\"training {layer_number} hidden layer model with hyper_number {hyper_number} and random_number {random_number}...\")\n",
    "            for epoch in range(n_epoch):\n",
    "                for itr, data in enumerate(loader_train):\n",
    "                    real_fps = data[0][:,:].to(device)\n",
    "                    real_spectrum = data[1][:,1:].to(device)\n",
    "                    optimizerG.zero_grad()\n",
    "\n",
    "                    pred_spectrum = netG(real_fps)\n",
    "\n",
    "                    mu_parameter = netG.mu\n",
    "                    mu_para_diff = torch.diff(mu_parameter)\n",
    "                    delta_mu = torch.sum(torch.pow((mu_para_diff),2))\n",
    "                    \n",
    "                    loss = criterion(pred_spectrum, real_spectrum[:,:]) + regu * delta_mu\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizerG.step()\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                    if epoch % display_interval == 0 and itr % display_interval == 0:\n",
    "                        print('[{}/{}][{}/{}] Loss: {:.7f} '.format(epoch + 1, n_epoch, itr + 1, len(loader_train), loss.item()))\n",
    "                        \n",
    "            # save model\n",
    "            PATH_G = '{}/generator_time{:03d}_layer{:03d}_hyper{:03d}.pth'.format(model_path, random_number, layer_number, hyper_number) \n",
    "            torch.save(netG.state_dict(), PATH_G)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-appendix",
   "metadata": {},
   "source": [
    "### Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "brutal-solution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "##     output results  ##\n",
      "#####################\n",
      "GNet(\n",
      "  (fc_layers): SequentialLinear(\n",
      "    (layer_0): LinearLayer(\n",
      "      (linear): Linear(in_features=1024, out_features=896, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (normalizer): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (layer_1): LinearLayer(\n",
      "      (linear): Linear(in_features=896, out_features=640, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (normalizer): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (layer_2): LinearLayer(\n",
      "      (linear): Linear(in_features=640, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (normalizer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (layer_3): LinearLayer(\n",
      "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (normalizer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (output): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      ")\n",
      "Median of RMSE: 0.11378708 std: 0.022274338\n",
      "Median of R square: 0.8017037513710209 std: 0.07806443989819846\n",
      "Median of MAE: 0.07744441 std: 0.012547183\n",
      "Median of RMSE_derivative 0.012247709 std: 0.001072027\n"
     ]
    }
   ],
   "source": [
    "# repeat for different number of layers in the neural network\n",
    "for layer_number in range(start_layer, end_layer):\n",
    "    # repeat for different sigma values for the kernel\n",
    "    for hyper_number in range(start_hyper, end_hyper):\n",
    "        # initializing variables for performance evaluation\n",
    "        anal_index = []        \n",
    "        anal_real_list = []\n",
    "        anal_pred_list = []\n",
    "\n",
    "        peak_position_real_list = []\n",
    "        peak_position_pred_list = []\n",
    "        peak_intensity_real_list = []\n",
    "        peak_intensity_pred_list = []\n",
    "\n",
    "        rmse_median = []\n",
    "        rmse_data = []\n",
    "        rsquare_median = []\n",
    "        mae_median = []\n",
    "        drmse_median = []\n",
    "        \n",
    "        # repeat for three times to check sensitivity to data splitting\n",
    "        for random_number in range(3):\n",
    "            # split data with different random seed\n",
    "            X_train, X_test_val, Y_train, Y_test_val = train_test_split(X_data, Y_data, test_size=split_size, random_state=random_number) \n",
    "            X_test, X_val, Y_test, Y_val = train_test_split(X_test_val, Y_test_val, test_size=split_val_size, random_state=random_number)\n",
    "            Dataset_train = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "            loader_train = torch.utils.data.DataLoader(Dataset_train, batch_size=50, shuffle=True)\n",
    "            Dataset_val = torch.utils.data.TensorDataset(X_val, Y_val)\n",
    "            loader_val = torch.utils.data.DataLoader(Dataset_val, shuffle=False)\n",
    "\n",
    "            # bulid model\n",
    "            netG = KernelNet(\n",
    "                n_neurons=n_neurons[layer_number - 1],\n",
    "                wavelength_points=wavelength_points,\n",
    "                kernel_grids=kernel_grids,\n",
    "                kernel_func=RBFKernel(\n",
    "                    sigmas_squared=kernel_hypers[hyper_number - 1][1],\n",
    "                    hight=kernel_hypers[hyper_number - 1][0]\n",
    "                ),\n",
    "            ).to(device)\n",
    "    \n",
    "            #########\n",
    "            ## Load trained model\n",
    "            #########              \n",
    "            trained_netG_path = '{}/generator_time{:03d}_layer{:03d}_hyper{:03d}.pth'.format(model_path, random_number, layer_number, hyper_number)\n",
    "            netG.load_state_dict(torch.load(trained_netG_path))     \n",
    "\n",
    "            ############\n",
    "            ### Analysis\n",
    "            ############\n",
    "            pred_spectrum = netG(X_test.to(device))\n",
    "            anal_real = Y_test[:,1:].to('cpu').detach().numpy().copy()\n",
    "            anal_pred = pred_spectrum[:].to('cpu').detach().numpy().copy()\n",
    "            anal_index.extend(Y_test[:,0].to('cpu').detach().numpy().copy())\n",
    "            anal_real_list.extend(anal_real)\n",
    "            anal_pred_list.extend(anal_pred)\n",
    "\n",
    "            rmse_list = []\n",
    "            rsquare_list = []\n",
    "            mae_list = []\n",
    "            drmse_list = []\n",
    "        \n",
    "            for i in range(anal_real.shape[0]):\n",
    "                # RMSE\n",
    "                rmse = np.sqrt(mean_squared_error(anal_real[i], anal_pred[i]))\n",
    "                rmse_list.append(rmse)\n",
    "                rmse_data.append(rmse)\n",
    "                # R square\n",
    "                rsquare = r2_score(anal_real[i], anal_pred[i])\n",
    "                rsquare_list.append(rsquare)\n",
    "                # MAE\n",
    "                mae = mean_absolute_error(anal_real[i], anal_pred[i])\n",
    "                mae_list.append(mae)\n",
    "            # RMSE derivative\n",
    "            real_d = anal_real[:,:-1] - anal_real[:,1:]\n",
    "            pred_d = anal_pred[:,:-1] - anal_pred[:,1:]\n",
    "            drmse = np.sqrt(np.sum(((real_d - pred_d) ** 2), axis=1) / anal_real.shape[0])\n",
    "            drmse_list.append(drmse)\n",
    "            # calculate median\n",
    "            rmse_median.append(np.median(rmse_list))\n",
    "            rsquare_median.append(np.median(rsquare_list))\n",
    "            mae_median.append(np.median(mae_list))\n",
    "            drmse_median.append(np.median(drmse_list))\n",
    "\n",
    "        # all test results\n",
    "        anal_index = np.array(anal_index, dtype='int32')\n",
    "        anal_real_list = np.array(anal_real_list, dtype='float32')\n",
    "        anal_pred_list = np.array(anal_pred_list, dtype='float32')\n",
    "\n",
    "\n",
    "        print(\"#####################\")\n",
    "        print(\"##     output results  ##\")\n",
    "        print(\"#####################\")\n",
    "        print(netG)\n",
    "\n",
    "        # output profiles\n",
    "        device_cpu = torch.device('cpu')\n",
    "        exp = pd.DataFrame(anal_real_list)\n",
    "        exp.insert(loc = 0, column='smiles', value= smiles_list[anal_index])\n",
    "        if case == 1:\n",
    "            exp.to_csv('{}/exp_profiles_dataset1.csv'.format(out_result))\n",
    "        elif case == 2:\n",
    "            exp.to_csv('{}/exp_profiles_dataset2.csv'.format(out_result))\n",
    "        pred = pd.DataFrame(anal_pred_list)\n",
    "        pred.insert(loc = 0, column='smiles', value= smiles_list[anal_index])\n",
    "        if case == 1:\n",
    "            exp.to_csv('{}/pred_profiles_dataset1.csv'.format(out_result))\n",
    "        elif case == 2:\n",
    "            exp.to_csv('{}/pred_profiles_dataset2.csv'.format(out_result))\n",
    "            \n",
    "        # RMSE        \n",
    "        rmse_df = pd.DataFrame([rmse_data])\n",
    "        rmse_df = rmse_df.transpose()\n",
    "        rmse_df.columns = [\"rmse\"]\n",
    "        rmse_sort = rmse_df.sort_values('rmse')\n",
    "        rmse_sort_index = rmse_sort.index.values\n",
    "        print(\"Median of RMSE:\", np.mean(rmse_median), \"std:\", np.std(rmse_median))\n",
    "        # R2\n",
    "        print(\"Median of R square:\", np.mean(rsquare_median), \"std:\", np.std(rsquare_median))\n",
    "        # MAE\n",
    "        print(\"Median of MAE:\", np.mean(mae_median), \"std:\",np.std(mae_median))            \n",
    "        # dRMSE\n",
    "        print(\"Median of RMSE_derivative\", np.mean(drmse_median), \"std:\",np.std(drmse_median))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-holmes",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
