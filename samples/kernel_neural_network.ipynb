{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "102ce65d-e20a-4def-9fa6-44ce040f47ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from rdkit.Avalon import pyAvalonTools\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit import rdBase, Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.AtomPairs import Pairs, Torsions\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# user-friendly print\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b938543e-a32b-4922-8f20-ea8373d669c6",
   "metadata": {},
   "source": [
    "### Util function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "069bb99a-8107-4d3a-9c95-99c864a4b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import Union, Sequence, Callable, Tuple\n",
    "\n",
    "\n",
    "class RBFKernel():\n",
    "    def __init__(self, sigmas_squared: Union[float, int, np.ndarray, Sequence], hight=10, *, dtype='float32'):\n",
    "        \"\"\"\n",
    "        Radial Basis Function (RBF) kernel function.\n",
    "        https://en.wikipedia.org/wiki/Radial_basis_function_kernel\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sigmas:\n",
    "            The squared standard deviations (SD).\n",
    "            Can be a single number or a 1d array-like object.\n",
    "        x_i: np.ndarray\n",
    "            Should be a 1d array.\n",
    "        x_j: np.ndarray\n",
    "            Should be a 1d array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Distribution under RBF kernel.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            Raise error if sigmas has wrong dimension.\n",
    "        \"\"\"\n",
    "        sigmas_squared = np.asarray(sigmas_squared)\n",
    "        if sigmas_squared.ndim == 0:\n",
    "            sigmas_squared = sigmas_squared[np.newaxis]\n",
    "        if sigmas_squared.ndim != 1:\n",
    "            raise ValueError('parameter `sigmas_squared` must be a array-like object which has dimension 1')\n",
    "        self._sigmas_squared = sigmas_squared\n",
    "        self.hight = hight\n",
    "        self.dtype = dtype\n",
    "        \n",
    "    @property\n",
    "    def sigmas(self):\n",
    "        return np.copy(self._sigmas)\n",
    "    \n",
    "    def __call__(self, x_i: np.ndarray, x_j: Union[np.ndarray, int, float]):\n",
    "        # K(x_i, x_j) = exp(-||x_i - x_j||^2 / (2 * sigma^2))\n",
    "        p1 = np.power(np.expand_dims(x_i, axis=x_i.ndim) - x_j, 2)\n",
    "        p2 = self._sigmas_squared * 2\n",
    "        dists = self.hight * np.exp(-np.expand_dims(p1, axis=p1.ndim) / p2)\n",
    "\n",
    "        if dists.shape[2] == 1:\n",
    "            return dists[:, :, 0].astype(self.dtype)\n",
    "        return dists.astype(self.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e54640b-9b9d-4dc9-af56-2853bd633b19",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "0f9296c4-f8cc-40dd-9e2a-1d200d39f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xenonpy.model import SequentialLinear\n",
    "\n",
    "@torch.no_grad()\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        \n",
    "class KernelNet(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                 n_neurons=(1024, 896, 512, 256, 128),\n",
    "                 activation_func=nn.LeakyReLU(0.2, inplace=True),\n",
    "                 kernel_grids: Union[int, np.ndarray] = 128,\n",
    "                 wavelength_points: Union[int, np.ndarray] =181,\n",
    "                 kernel_func=RBFKernel(sigmas_squared=0.05, hight=1),\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        # none training params\n",
    "        if isinstance(kernel_grids, int):\n",
    "            kernel_grids = np.linspace(0, 1, kernel_grids)\n",
    "        if isinstance(kernel_grids, np.ndarray):\n",
    "            self.kernel_grids = kernel_grids\n",
    "        else:\n",
    "            raise ValueError('kernel_grids error!')\n",
    "        \n",
    "        if isinstance(wavelength_points, int):\n",
    "            wavelength_points = np.linspace(0, 1, wavelength_points)\n",
    "        if isinstance(wavelength_points, np.ndarray):\n",
    "            self.wavelength_points = wavelength_points\n",
    "        else:\n",
    "            raise ValueError('wavelength_points error!')\n",
    "        \n",
    "        # FC layers\n",
    "        self.fc_layers = SequentialLinear(\n",
    "            in_features=n_neurons[0], out_features=n_neurons[-1], h_neurons=n_neurons[1:-1], h_activation_funcs=activation_func\n",
    "        )\n",
    "        \n",
    "        # baseline parameters\n",
    "        self.mu = torch.nn.Parameter(torch.zeros(wavelength_points.size))\n",
    "        \n",
    "        # kernel\n",
    "        self.kernel =  torch.from_numpy(\n",
    "            kernel_func(self.kernel_grids, self.wavelength_points)\n",
    "        )\n",
    "        \n",
    "    def to(self, *arg, **kwarg):\n",
    "        self.kernel = self.kernel.to(*arg, **kwarg)\n",
    "        return super().to(*arg, **kwarg)\n",
    "\n",
    "    def forward(self, fingerprint_input):\n",
    "        x = self.fc_layers(fingerprint_input)\n",
    "        x = x.view(x.size(0), self.kernel_grids.size, -1) \n",
    "        x = x * self.kernel\n",
    "        x = torch.sum(x, dim=1)\n",
    "        x = self.mu + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c8211-9bbc-452b-9c02-166f8756cdd4",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "93b76912-f11a-49e6-9843-8830e47784b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data: 949\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data/Dataset_I.csv\"\n",
    "input_file = pd.read_csv(data_file,sep=\",\")\n",
    "\n",
    "# Figure\n",
    "neural_network = Path(\"spectrum_results\")\n",
    "out_result.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "# drop \"OC(=O)C(=C\\C1=CC=[Cl]C=[Cl]1)\\C1=CN=CC=C1\" because of Explicit valence error\n",
    "input_file = input_file[input_file.SMILES != \"OC(=O)C(=C\\C1=CC=[Cl]C=[Cl]1)\\C1=CN=CC=C1\"]\n",
    "\n",
    "# load smiles and spectrum\n",
    "file_smiles_spectrum = input_file.values\n",
    "smiles_list = (file_smiles_spectrum[:,1])\n",
    "spectrum_list = file_smiles_spectrum[:,2:]\n",
    "spectrum_list = np.array(spectrum_list, dtype='float32')\n",
    "\n",
    "print(\"number of data:\", len(smiles_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d6acc-2f24-4f1c-8604-ec453abb4def",
   "metadata": {},
   "source": [
    "#### convert smiles to fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59c214c0-d2ef-4911-83e2-6219c67b8c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([949, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([949, 182])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert smiles to mol file\n",
    "mol_list = [Chem.MolFromSmiles(s) for s in smiles_list if s is not None]\n",
    "    \n",
    "## Morgan finger print (ECFP)\n",
    "fps_bit = [AllChem.GetMorganFingerprintAsBitVect(m, 3, 1024) for m in mol_list]\n",
    "fps_list = []\n",
    "for fps in fps_bit:\n",
    "    fps_arr = np.zeros((1,))\n",
    "    DataStructs.ConvertToNumpyArray(fps, fps_arr)\n",
    "    fps_list.append(fps_arr)\n",
    "fps_list = np.array(fps_list, dtype='float32')\n",
    "\n",
    "fps_list_max = np.amax(fps_list)\n",
    "fps_list_min = np.amin(fps_list)\n",
    "fps_norm = ((fps_list - fps_list_min) / (fps_list_max - fps_list_min))\n",
    "X_data = torch.from_numpy(fps_norm)\n",
    "\n",
    "\n",
    "spectrum_list_max = np.amax(spectrum_list)\n",
    "spectrum_list_min = np.amin(spectrum_list)\n",
    "spectrum_norm = ((spectrum_list - spectrum_list_min) / (spectrum_list_max - spectrum_list_min))\n",
    "Y_data = torch.from_numpy(spectrum_norm)\n",
    "data_number = torch.zeros((X_data.shape[0],1))\n",
    "Y_data = torch.cat((data_number, Y_data), dim=1)\n",
    "for i in range(Y_data.shape[0]):\n",
    "    number = torch.tensor([i]) \n",
    "    Y_data[i][0] = number\n",
    "    \n",
    "X_data.shape\n",
    "Y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6c887c-3046-44f3-a825-6c129cfbd80c",
   "metadata": {},
   "source": [
    "### hyper-params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc94c6a-026b-4533-a795-95fb988f0244",
   "metadata": {},
   "source": [
    "#### training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "1aba1d3c-e7f3-48b3-a846-b2508b45a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 3000 # epoch\n",
    "lr = 0.0002 # learning rate\n",
    "ngpu = 2 # number of GPU\n",
    "display_interval = 100\n",
    "\n",
    "model_path = Path('model/dataset1')\n",
    "model_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "03782402-a11e-45cb-85da-55cb59db94a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "best_layer_number = 4 # number of layers\n",
    "best_variance = 0.0005 # the variange of RBF kernel\n",
    "best_length = 0.5 # the length of RBF kernel \n",
    "\n",
    "# split train:val:test\n",
    "split_size = 0.316\n",
    "split_val_size = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "db50d283-9d2e-4296-9a6d-7744595197d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_layer = best_layer_number\n",
    "end_layer = start_layer + 1\n",
    "\n",
    "hyper_id = [5, 6, 7, 8]\n",
    "hyper_order = 3\n",
    "start_hyper = hyper_id[hyper_order]\n",
    "end_hyper = start_hyper + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73114165-d9c8-4fd2-bae1-a3248eafd393",
   "metadata": {},
   "source": [
    "#### model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "8bf3e1ab-da0e-4048-a32a-622cbf89d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength_points = 181\n",
    "kernel_grids = 128\n",
    "n_neurons = [\n",
    "    (1024, 512, 128),                 # 1 hidden layer\n",
    "    (1024, 512, 256, 128),            # 2 hidden layer\n",
    "    (1024, 896, 512, 256, 128),       # 3 hidden layer\n",
    "    (1024, 896, 640, 512, 256, 128),  # 4 hidden layer\n",
    "]\n",
    "kernel_hypers = (\n",
    "    [10, 0.00005], [5, 0.00005], [1, 0.00005], [0.5, 0.00005],\n",
    "    [10, 0.0005], [5, 0.0005], [1, 0.0005], [0.5, 0.0005],\n",
    "    [10, 0.00025], [5, 0.00025], [1, 0.00025], [0.5, 0.00025],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1f5b4920-3806-4c3f-9918-d2d5c4048b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x145f7344f1f0>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f0018-e560-416f-a6b7-de591e31dec2",
   "metadata": {},
   "source": [
    "### model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "36e47944-2359-4e2f-afdd-269c2efaea64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNet(\n",
       "  (fc_layers): SequentialLinear(\n",
       "    (layer_0): LinearLayer(\n",
       "      (linear): Linear(in_features=1024, out_features=896, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_1): LinearLayer(\n",
       "      (linear): Linear(in_features=896, out_features=640, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_2): LinearLayer(\n",
       "      (linear): Linear(in_features=640, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_3): LinearLayer(\n",
       "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (output): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 4 hidden layer model with hyper_number 8 and random_number 0...\n",
      "[1/3000][1/13] Loss: 0.9513877 \n",
      "[101/3000][1/13] Loss: 0.0049158 \n",
      "[201/3000][1/13] Loss: 0.0015354 \n",
      "[301/3000][1/13] Loss: 0.0025721 \n",
      "[401/3000][1/13] Loss: 0.0016615 \n",
      "[501/3000][1/13] Loss: 0.0015076 \n",
      "[601/3000][1/13] Loss: 0.0017643 \n",
      "[701/3000][1/13] Loss: 0.0011007 \n",
      "[801/3000][1/13] Loss: 0.0012107 \n",
      "[901/3000][1/13] Loss: 0.0005568 \n",
      "[1001/3000][1/13] Loss: 0.0009820 \n",
      "[1101/3000][1/13] Loss: 0.0014051 \n",
      "[1201/3000][1/13] Loss: 0.0009783 \n",
      "[1301/3000][1/13] Loss: 0.0006637 \n",
      "[1401/3000][1/13] Loss: 0.0008219 \n",
      "[1501/3000][1/13] Loss: 0.0006080 \n",
      "[1601/3000][1/13] Loss: 0.0007286 \n",
      "[1701/3000][1/13] Loss: 0.0009694 \n",
      "[1801/3000][1/13] Loss: 0.0008945 \n",
      "[1901/3000][1/13] Loss: 0.0005525 \n",
      "[2001/3000][1/13] Loss: 0.0005456 \n",
      "[2101/3000][1/13] Loss: 0.0008501 \n",
      "[2201/3000][1/13] Loss: 0.0004946 \n",
      "[2301/3000][1/13] Loss: 0.0003780 \n",
      "[2401/3000][1/13] Loss: 0.0005973 \n",
      "[2501/3000][1/13] Loss: 0.0005007 \n",
      "[2601/3000][1/13] Loss: 0.0006730 \n",
      "[2701/3000][1/13] Loss: 0.0005046 \n",
      "[2801/3000][1/13] Loss: 0.0003679 \n",
      "[2901/3000][1/13] Loss: 0.0004459 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GNet(\n",
       "  (fc_layers): SequentialLinear(\n",
       "    (layer_0): LinearLayer(\n",
       "      (linear): Linear(in_features=1024, out_features=896, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_1): LinearLayer(\n",
       "      (linear): Linear(in_features=896, out_features=640, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_2): LinearLayer(\n",
       "      (linear): Linear(in_features=640, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_3): LinearLayer(\n",
       "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (output): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 4 hidden layer model with hyper_number 8 and random_number 1...\n",
      "[1/3000][1/13] Loss: 0.8627182 \n",
      "[101/3000][1/13] Loss: 0.0093594 \n",
      "[201/3000][1/13] Loss: 0.0030646 \n",
      "[301/3000][1/13] Loss: 0.0041787 \n",
      "[401/3000][1/13] Loss: 0.0021683 \n",
      "[501/3000][1/13] Loss: 0.0025061 \n",
      "[601/3000][1/13] Loss: 0.0014158 \n",
      "[701/3000][1/13] Loss: 0.0016073 \n",
      "[801/3000][1/13] Loss: 0.0020572 \n",
      "[901/3000][1/13] Loss: 0.0013698 \n",
      "[1001/3000][1/13] Loss: 0.0013439 \n",
      "[1101/3000][1/13] Loss: 0.0011626 \n",
      "[1201/3000][1/13] Loss: 0.0013354 \n",
      "[1301/3000][1/13] Loss: 0.0007292 \n",
      "[1401/3000][1/13] Loss: 0.0008215 \n",
      "[1501/3000][1/13] Loss: 0.0005767 \n",
      "[1601/3000][1/13] Loss: 0.0007712 \n",
      "[1701/3000][1/13] Loss: 0.0009511 \n",
      "[1801/3000][1/13] Loss: 0.0006395 \n",
      "[1901/3000][1/13] Loss: 0.0005922 \n",
      "[2001/3000][1/13] Loss: 0.0004482 \n",
      "[2101/3000][1/13] Loss: 0.0015007 \n",
      "[2201/3000][1/13] Loss: 0.0005241 \n",
      "[2301/3000][1/13] Loss: 0.0005661 \n",
      "[2401/3000][1/13] Loss: 0.0005935 \n",
      "[2501/3000][1/13] Loss: 0.0004629 \n",
      "[2601/3000][1/13] Loss: 0.0005072 \n",
      "[2701/3000][1/13] Loss: 0.0011309 \n",
      "[2801/3000][1/13] Loss: 0.0006281 \n",
      "[2901/3000][1/13] Loss: 0.0004993 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GNet(\n",
       "  (fc_layers): SequentialLinear(\n",
       "    (layer_0): LinearLayer(\n",
       "      (linear): Linear(in_features=1024, out_features=896, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_1): LinearLayer(\n",
       "      (linear): Linear(in_features=896, out_features=640, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_2): LinearLayer(\n",
       "      (linear): Linear(in_features=640, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (layer_3): LinearLayer(\n",
       "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (normalizer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (output): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 4 hidden layer model with hyper_number 8 and random_number 2...\n",
      "[1/3000][1/13] Loss: 1.0644132 \n",
      "[101/3000][1/13] Loss: 0.0099005 \n",
      "[201/3000][1/13] Loss: 0.0022077 \n",
      "[301/3000][1/13] Loss: 0.0020309 \n",
      "[401/3000][1/13] Loss: 0.0017221 \n",
      "[501/3000][1/13] Loss: 0.0014918 \n",
      "[601/3000][1/13] Loss: 0.0014006 \n",
      "[701/3000][1/13] Loss: 0.0013380 \n",
      "[801/3000][1/13] Loss: 0.0011670 \n",
      "[901/3000][1/13] Loss: 0.0013053 \n",
      "[1001/3000][1/13] Loss: 0.0006377 \n",
      "[1101/3000][1/13] Loss: 0.0009901 \n",
      "[1201/3000][1/13] Loss: 0.0008227 \n",
      "[1301/3000][1/13] Loss: 0.0011035 \n",
      "[1401/3000][1/13] Loss: 0.0006691 \n",
      "[1501/3000][1/13] Loss: 0.0009261 \n",
      "[1601/3000][1/13] Loss: 0.0007164 \n",
      "[1701/3000][1/13] Loss: 0.0007345 \n",
      "[1801/3000][1/13] Loss: 0.0006400 \n",
      "[1901/3000][1/13] Loss: 0.0005412 \n",
      "[2001/3000][1/13] Loss: 0.0003890 \n",
      "[2101/3000][1/13] Loss: 0.0004704 \n",
      "[2201/3000][1/13] Loss: 0.0003938 \n",
      "[2301/3000][1/13] Loss: 0.0004376 \n",
      "[2401/3000][1/13] Loss: 0.0005553 \n",
      "[2501/3000][1/13] Loss: 0.0007593 \n",
      "[2601/3000][1/13] Loss: 0.0003920 \n",
      "[2701/3000][1/13] Loss: 0.0003260 \n",
      "[2801/3000][1/13] Loss: 0.0003903 \n",
      "[2901/3000][1/13] Loss: 0.0005001 \n"
     ]
    }
   ],
   "source": [
    "for layer_number in range(start_layer, end_layer):\n",
    "\n",
    "    for hyper_number in range(start_hyper, end_hyper):\n",
    "        \n",
    "        for random_number in range(3):\n",
    "            ## split data\n",
    "            X_train, X_test_val, Y_train, Y_test_val = train_test_split(X_data, Y_data, test_size=split_size, random_state=random_number) \n",
    "            X_test, X_val, Y_test, Y_val = train_test_split(X_test_val, Y_test_val, test_size=split_val_size, random_state=random_number)\n",
    "            Dataset_train = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "            loader_train = torch.utils.data.DataLoader(Dataset_train, batch_size=50, shuffle=True)\n",
    "            Dataset_val = torch.utils.data.TensorDataset(X_val, Y_val)\n",
    "            loader_val = torch.utils.data.DataLoader(Dataset_val, shuffle=False)\n",
    "\n",
    "            # bulid model\n",
    "            netG = KernelNet(\n",
    "                n_neurons=n_neurons[layer_number - 1],\n",
    "                wavelength_points=wavelength_points,\n",
    "                kernel_grids=kernel_grids,\n",
    "                kernel_func=RBFKernel(\n",
    "                    sigmas_squared=kernel_hypers[hyper_number - 1][1],\n",
    "                    hight=kernel_hypers[hyper_number - 1][0]\n",
    "                ),\n",
    "            ).to(device)\n",
    "\n",
    "            # Handle multi-gpu if desired\n",
    "\n",
    "            # if (device.type == 'cuda') and (torch.cuda.device_count() > ngpu):\n",
    "            #     netG = nn.DataParallel(netG, device_ids=range(ngpu))            \n",
    "            netG = netG.apply(weights_init)\n",
    "\n",
    "            ## Train\n",
    "            # regularization parameter\n",
    "            regu = torch.Tensor([1]).to(device)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "            \n",
    "            losses = [] \n",
    "            print(f\"training {layer_number} hidden layer model with hyper_number {hyper_number} and random_number {random_number}...\")\n",
    "            for epoch in range(n_epoch):\n",
    "                for itr, data in enumerate(loader_train):\n",
    "                    real_fps = data[0][:,:].to(device)\n",
    "                    real_spectrum = data[1][:,1:].to(device)\n",
    "                    optimizerG.zero_grad()\n",
    "\n",
    "                    pred_spectrum = netG(real_fps)\n",
    "\n",
    "                    mu_parameter = netG.mu\n",
    "                    mu_para_diff = torch.diff(mu_parameter)\n",
    "                    delta_mu = torch.sum(torch.pow((mu_para_diff),2))\n",
    "                    \n",
    "                    loss = criterion(pred_spectrum, real_spectrum[:,:]) + regu * delta_mu\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizerG.step()\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                    if epoch % display_interval == 0 and itr % display_interval == 0:\n",
    "                        print('[{}/{}][{}/{}] Loss: {:.7f} '.format(epoch + 1, n_epoch, itr + 1, len(loader_train), loss.item()))\n",
    "                        \n",
    "            # save model\n",
    "            PATH_G = '{}/generator_time{:03d}_layer{:03d}_hyper{:03d}.pth'.format(model_path, random_number, layer_number, hyper_number) \n",
    "            torch.save(netG.state_dict(), PATH_G)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34190e9-525a-47a5-97be-e93f033caf33",
   "metadata": {},
   "source": [
    "#### test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "94591351-686c-4fb9-9e29-e2e1bbc9b209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "##     output results  ##\n",
      "#####################\n",
      "GNet(\n",
      "  (fc_layers): SequentialLinear(\n",
      "    (layer_0): LinearLayer(\n",
      "      (linear): Linear(in_features=1024, out_features=896, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (normalizer): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (layer_1): LinearLayer(\n",
      "      (linear): Linear(in_features=896, out_features=640, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (normalizer): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (layer_2): LinearLayer(\n",
      "      (linear): Linear(in_features=640, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (normalizer): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (layer_3): LinearLayer(\n",
      "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (normalizer): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (output): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      ")\n",
      "Median of RMSE: 0.11378708 std: 0.022274338\n",
      "Median of R square: 0.8017037513710209 std: 0.07806443989819846\n",
      "Median of MAE: 0.07744441 std: 0.012547183\n",
      "Median of RMSE_derivative 0.012247709 std: 0.001072027\n"
     ]
    }
   ],
   "source": [
    "for layer_number in range(start_layer, end_layer):\n",
    "\n",
    "    for hyper_number in range(start_hyper, end_hyper):\n",
    "        anal_index = []        \n",
    "        anal_real_list = []\n",
    "        anal_pred_list = []\n",
    "\n",
    "        peak_position_real_list = []\n",
    "        peak_position_pred_list = []\n",
    "        peak_intensity_real_list = []\n",
    "        peak_intensity_pred_list = []\n",
    "\n",
    "\n",
    "        rmse_median = []\n",
    "        rmse_data = []\n",
    "        rsquare_median = []\n",
    "        mae_median = []\n",
    "        drmse_median = []\n",
    "        \n",
    "        for random_number in range(3):\n",
    "            ## split data\n",
    "            X_train, X_test_val, Y_train, Y_test_val = train_test_split(X_data, Y_data, test_size=split_size, random_state=random_number) \n",
    "            X_test, X_val, Y_test, Y_val = train_test_split(X_test_val, Y_test_val, test_size=split_val_size, random_state=random_number)\n",
    "            Dataset_train = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "            loader_train = torch.utils.data.DataLoader(Dataset_train, batch_size=50, shuffle=True)\n",
    "            Dataset_val = torch.utils.data.TensorDataset(X_val, Y_val)\n",
    "            loader_val = torch.utils.data.DataLoader(Dataset_val, shuffle=False)\n",
    "\n",
    "            # bulid model\n",
    "            netG = KernelNet(\n",
    "                n_neurons=n_neurons[layer_number - 1],\n",
    "                wavelength_points=wavelength_points,\n",
    "                kernel_grids=kernel_grids,\n",
    "                kernel_func=RBFKernel(\n",
    "                    sigmas_squared=kernel_hypers[hyper_number - 1][1],\n",
    "                    hight=kernel_hypers[hyper_number - 1][0]\n",
    "                ),\n",
    "            ).to(device)\n",
    "    \n",
    "            #########\n",
    "            ## Load trained model\n",
    "            #########              \n",
    "            trained_netG_path = '{}/generator_time{:03d}_layer{:03d}_hyper{:03d}.pth'.format(import_model, random_number, layer_number, hyper_number)\n",
    "            netG.load_state_dict(torch.load(trained_netG_path))     \n",
    "\n",
    "            ############\n",
    "            ### Analysis\n",
    "            ############\n",
    "            pred_spectrum = netG(X_test.to(device))\n",
    "            anal_real = Y_test[:,1:].to('cpu').detach().numpy().copy()\n",
    "            anal_pred = pred_spectrum[:].to('cpu').detach().numpy().copy()\n",
    "            anal_index.extend(Y_test[:,0].to('cpu').detach().numpy().copy())\n",
    "            anal_real_list.extend(anal_real)\n",
    "            anal_pred_list.extend(anal_pred)\n",
    "\n",
    "            rmse_list = []\n",
    "            rsquare_list = []\n",
    "            mae_list = []\n",
    "            drmse_list = []\n",
    "        \n",
    "            for i in range(anal_real.shape[0]):\n",
    "                # RMSE\n",
    "                rmse = np.sqrt(mean_squared_error(anal_real[i], anal_pred[i]))\n",
    "                rmse_list.append(rmse)\n",
    "                rmse_data.append(rmse)\n",
    "                # R square\n",
    "                rsquare = r2_score(anal_real[i], anal_pred[i])\n",
    "                rsquare_list.append(rsquare)\n",
    "                # MAE\n",
    "                mae = mean_absolute_error(anal_real[i], anal_pred[i])\n",
    "                mae_list.append(mae)\n",
    "            # RMSE_derivative\n",
    "            real_d = anal_real[:,:-1] - anal_real[:,1:]\n",
    "            pred_d = anal_pred[:,:-1] - anal_pred[:,1:]\n",
    "            drmse = np.sqrt(np.sum(((real_d - pred_d) ** 2), axis=1) / anal_real.shape[0])\n",
    "            drmse_list.append(drmse)\n",
    "            # calc Median\n",
    "            rmse_median.append(np.median(rmse_list))\n",
    "            rsquare_median.append(np.median(rsquare_list))\n",
    "            mae_median.append(np.median(mae_list))\n",
    "            drmse_median.append(np.median(drmse_list))\n",
    "\n",
    "        # all test results\n",
    "        anal_index = np.array(anal_index, dtype='int32')\n",
    "        anal_real_list = np.array(anal_real_list, dtype='float32')\n",
    "        anal_pred_list = np.array(anal_pred_list, dtype='float32')\n",
    "\n",
    "\n",
    "        print(\"#####################\")\n",
    "        print(\"##     output results  ##\")\n",
    "        print(\"#####################\")\n",
    "        print(netG)\n",
    "\n",
    "        # output profiles\n",
    "        device_cpu = torch.device('cpu')\n",
    "        exp = pd.DataFrame(anal_real_list)\n",
    "        exp.insert(loc = 0, column='smiles', value= smiles_list[anal_index])\n",
    "        if case == 1:\n",
    "            exp.to_csv('{}/exp_profiles_dataset1.csv'.format(out_result))\n",
    "        elif case == 2:\n",
    "            exp.to_csv('{}/exp_profiles_dataset2.csv'.format(out_result))\n",
    "        pred = pd.DataFrame(anal_pred_list)\n",
    "        pred.insert(loc = 0, column='smiles', value= smiles_list[anal_index])\n",
    "        if case == 1:\n",
    "            exp.to_csv('{}/pred_profiles_dataset1.csv'.format(out_result))\n",
    "        elif case == 2:\n",
    "            exp.to_csv('{}/pred_profiles_dataset2.csv'.format(out_result))\n",
    "\n",
    "            \n",
    "        # RMSE        \n",
    "        rmse_df = pd.DataFrame([rmse_data])\n",
    "        rmse_df = rmse_df.transpose()\n",
    "        rmse_df.columns = [\"rmse\"]\n",
    "        rmse_sort = rmse_df.sort_values('rmse')\n",
    "        rmse_sort_index = rmse_sort.index.values\n",
    "        print(\"Median of RMSE:\", np.mean(rmse_median), \"std:\", np.std(rmse_median))\n",
    "        # R2\n",
    "        print(\"Median of R square:\", np.mean(rsquare_median), \"std:\", np.std(rsquare_median))\n",
    "        # MAE\n",
    "        print(\"Median of MAE:\", np.mean(mae_median), \"std:\",np.std(mae_median))            \n",
    "        # dRMSE\n",
    "        print(\"Median of RMSE_derivative\", np.mean(drmse_median), \"std:\",np.std(drmse_median))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9267bd8-de7b-4806-bd64-46643c35ad47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
